{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd7d52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this tutorial, I will show all operation required to create cluster and tables.\n",
    "\n",
    "# Make sure install boto3,pg8000,psycopg2 ,pandas using pip\n",
    "# Make sure have access key and secret key \n",
    "\n",
    "# we are createing subnet group,security group ,tables in redshift and also upload data from s3 into redshift cluster\n",
    "#clean the system drop tables ,subnet goup and security group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "#import psycopg2\n",
    "\n",
    "access_key = 'AKIAZ5JEHXXXXXXXXX'\n",
    "secret_key = 'LFsX2L4XXX/XjbSkxXG+XXXXXXXXXXX'\n",
    "\n",
    "vpc_id='vpc-6XXXXXbXX'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73eef1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created security group with ID: sg-00bd2ea78a35f0f8c\n"
     ]
    }
   ],
   "source": [
    "#create security group by passing vpc_id and group name\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#create security group\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "group_name = 'my-redshift-security-group'\n",
    "group_description = 'Security group for Redshift cluster access'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create the security group\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=group_name,\n",
    "        Description=group_description,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "    print('Created security group with ID:', security_group_id)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'InvalidGroup.Duplicate':\n",
    "        # The security group already exists\n",
    "        response = ec2_client.describe_security_groups(\n",
    "            Filters=[\n",
    "                {'Name': 'group-name', 'Values': [group_name]},\n",
    "                {'Name': 'vpc-id', 'Values': [vpc_id]}\n",
    "            ]\n",
    "        )\n",
    "        security_group_id = response['SecurityGroups'][0]['GroupId']\n",
    "        print('Security group already exists. Using existing security group with ID:', security_group_id)\n",
    "    else:\n",
    "        # Handle other exceptions\n",
    "        print('Error creating security group:', e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sg-08f82fe187f58b3ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b17c55e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg-00bd2ea78a35f0f8c\n",
      "Inbound rule added to the security group.\n"
     ]
    }
   ],
   "source": [
    "#create inbound rule for security group\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "\n",
    "# Create an EC2 client\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "#security_group_id sg-08f82fe187f58b3ab  retun by above code\n",
    "print(security_group_id)  \n",
    "\n",
    "\n",
    "port = 5439\n",
    "ip_range = '0.0.0.0/0'\n",
    "\n",
    "try:\n",
    "    # Add the inbound rule to the security group\n",
    "    response = ec2_client.authorize_security_group_ingress(\n",
    "        GroupId=security_group_id,\n",
    "        IpPermissions=[\n",
    "            {\n",
    "                'IpProtocol': 'tcp',\n",
    "                'FromPort': port,\n",
    "                'ToPort': port,\n",
    "                'IpRanges': [{'CidrIp': ip_range}]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print('Inbound rule added to the security group.')\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'InvalidPermission.Duplicate':\n",
    "        print('Inbound rule already exists for the specified port and IP range.')\n",
    "    else:\n",
    "        print('Error adding inbound rule:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a17b2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subnet ID: subnet-e4129a9f\n",
      "VPC ID: vpc-6300eb08\n",
      "CIDR Block: 172.31.16.0/20\n",
      "Availability Zone: ap-south-1c\n",
      "---\n",
      "Subnet ID: subnet-100e4f5c\n",
      "VPC ID: vpc-6300eb08\n",
      "CIDR Block: 172.31.0.0/20\n",
      "Availability Zone: ap-south-1b\n",
      "---\n",
      "Subnet ID: subnet-1237277a\n",
      "VPC ID: vpc-6300eb08\n",
      "CIDR Block: 172.31.32.0/20\n",
      "Availability Zone: ap-south-1a\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# check vpc and subnet\n",
    "\n",
    "\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "# Fetch and print subnet details\n",
    "response = ec2_client.describe_subnets()\n",
    "\n",
    "for subnet in response['Subnets']:\n",
    "    subnet_id = subnet['SubnetId']\n",
    "    vpc_id = subnet['VpcId']\n",
    "    cidr_block = subnet['CidrBlock']\n",
    "    availability_zone = subnet['AvailabilityZone']\n",
    "\n",
    "    print(f\"Subnet ID: {subnet_id}\")\n",
    "    print(f\"VPC ID: {vpc_id}\")\n",
    "    print(f\"CIDR Block: {cidr_block}\")\n",
    "    print(f\"Availability Zone: {availability_zone}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44f83ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The create_cluster_subnet_group operation in Amazon Redshift is used to create a subnet group that \n",
    "# represents a group of subnets. When creating a Redshift cluster, \n",
    "# you are required to associate the cluster with a subnet group.\n",
    "\n",
    "# Deploying a cluster in multiple subnets allows you to distribute the cluster across \n",
    "# different availability zones (AZs). \n",
    "\n",
    "# If you have data sources or clients in different regions or AZs, placing the cluster in multiple subnets closer \n",
    "# to those data sources or clients can help reduce data transfer costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6dea9cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-subnet-group\n",
      "Subnet group created successfully.\n"
     ]
    }
   ],
   "source": [
    "#Need to create subnet group which conatins list of subnet\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key\n",
    "                              )\n",
    "\n",
    "subnet_group_name = 'my-subnet-group'\n",
    "subnet_ids = ['subnet-e4129a9f',\n",
    "              'subnet-100e4f5c',\n",
    "              'subnet-1237277a'\n",
    "             ]   # Replace with the appropriate subnet IDs\n",
    "\n",
    "try:\n",
    "    # Create the subnet group\n",
    "    response = redshift_client.create_cluster_subnet_group(\n",
    "        ClusterSubnetGroupName=subnet_group_name,\n",
    "        Description='My subnet group for redshift description',\n",
    "        SubnetIds=subnet_ids\n",
    "    )\n",
    "    print(subnet_group_name)\n",
    "    print('Subnet group created successfully.')\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ClusterSubnetGroupAlreadyExists':\n",
    "        print('Subnet group already exists. Skipping creation.')\n",
    "    else:\n",
    "        print('Error creating subnet group:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2ff1536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cluster parameters used to create cluster in redshift\n",
    "cluster_parameters = {\n",
    "    'ClusterIdentifier': 'my-redshift-cluster',\n",
    "    'NodeType': 'dc2.large',\n",
    "    'MasterUsername': 'myawsuser',\n",
    "    'MasterUserPassword': 'Password13',\n",
    "    'DBName': 'mydatabase',\n",
    "    'ClusterType': 'single-node',\n",
    "    'NumberOfNodes': 1,\n",
    "    'PubliclyAccessible': True,\n",
    "    'VpcSecurityGroupIds': [security_group_id],  # you take from above we already create security group\n",
    "    'AvailabilityZone': 'ap-south-1a', # primarily created in the specified availability zone.\n",
    "    'Port': 5439,\n",
    "    'ClusterSubnetGroupName': subnet_group_name    #created above wih name my-subnet-group\n",
    "     \n",
    "    # Add any other necessary cluster parameters here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef57aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift cluster creation initiated.\n",
      "Redshift cluster is now available.\n"
     ]
    }
   ],
   "source": [
    "#finally create cluster in redshift by passing cluster parameter\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "\n",
    "\n",
    "# Create the cluster\n",
    "try:\n",
    "    response = redshift_client.create_cluster(**cluster_parameters)\n",
    "    print('Redshift cluster creation initiated.')\n",
    "except redshift_client.exceptions.ClusterAlreadyExistsFault:\n",
    "    print('Cluster already exists. Skipping cluster creation.')\n",
    "    # You can choose to exit the program or perform other actions as needed\n",
    "    # exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ClusterIdentifier parameter specifies the unique identifier for your Redshift cluster.\n",
    "#The redshift_client.get_waiter('cluster_available').wait() statement waits until the Redshift cluster becomes available. \n",
    "#By default, it will continuously check the cluster status until it becomes available \n",
    "# Wait for the cluster to be available\n",
    "\n",
    "redshift_client.get_waiter('cluster_available').wait(\n",
    "    ClusterIdentifier=cluster_parameters['ClusterIdentifier']\n",
    ")\n",
    "\n",
    "print('Redshift cluster is now available.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b9e75e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::681365143658:role/mynewredshiftfortest\n"
     ]
    }
   ],
   "source": [
    "#I already created mynewredshiftfortest this role and have permission for s3 access to redshift.\n",
    "\n",
    "iam=boto3.client('iam',\n",
    "                  region_name='ap-south-1',\n",
    "                  aws_access_key_id=access_key,\n",
    "                  aws_secret_access_key=secret_key)\n",
    "\n",
    "roleArn=iam.get_role(RoleName='mynewredshiftfortest')['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e98beb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These roles will be granted permissions to access s3 within the cluster.\n"
     ]
    }
   ],
   "source": [
    "#modify_cluster_iam_roles method is used to modify the IAM roles associated with an Amazon Redshift cluster.\n",
    "s3_access_role_arn = roleArn\n",
    "\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "redshift_client.modify_cluster_iam_roles(\n",
    "    ClusterIdentifier=cluster_parameters['ClusterIdentifier'],\n",
    "    AddIamRoles=[s3_access_role_arn]\n",
    ")\n",
    "\n",
    "print('These roles will be granted permissions to access s3 within the cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "96aa75d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClusterIdentifier': 'my-redshift-cluster', 'NodeType': 'dc2.large', 'ClusterStatus': 'available', 'ClusterAvailabilityStatus': 'Available', 'MasterUsername': 'myawsuser', 'DBName': 'mydatabase', 'Endpoint': {'Address': 'my-redshift-cluster.cjoyi9o6hfqt.ap-south-1.redshift.amazonaws.com', 'Port': 5439}, 'ClusterCreateTime': datetime.datetime(2023, 7, 9, 18, 10, 15, 148000, tzinfo=tzutc()), 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-00bd2ea78a35f0f8c', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'my-subnet-group', 'VpcId': 'vpc-6300eb08', 'AvailabilityZone': 'ap-south-1a', 'PreferredMaintenanceWindow': 'sat:07:00-sat:07:30', 'PendingModifiedValues': {}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 1, 'PubliclyAccessible': True, 'Encrypted': False, 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCgQ6rEonpQ5Z5QE09qTPuF5F/YXWOXukfMmsCURvFlEW+pT7ZLYmAdKraITl3CAHYzPTDFI951e+yw0bD1VIrdH1WELhTV7gtiGFUp++2l54y2FDv7WnwJSYUD1xCGdKsdStZN5HDdSUBfleGnmI9BAr/4ItNvPrn3Z5hpkFO0ydaCTJySFhYt4DhtXvsZ2aaAJTOG3VITOdldKafLs5M/ZGjHQ1bJ80NzkG88MwFJy+A5p63l/nRGR71Eze5nqJj6v42jkPvTcokpVETMqTvfssgj7Zc8BsttnxzEuq3ey3kWLruk+dkrJyA00+p0iUVP5Z+HrJWp+QUYrUHHplbZ Amazon-Redshift\\n', 'ClusterNodes': [{'NodeRole': 'SHARED', 'PrivateIPAddress': '172.31.36.218', 'PublicIPAddress': '3.108.102.10'}], 'ClusterRevisionNumber': '52931', 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::681365143658:role/mynewredshiftfortest', 'ApplyStatus': 'in-sync'}], 'MaintenanceTrackName': 'current', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2023, 7, 15, 7, 0, tzinfo=tzutc()), 'AvailabilityZoneRelocationStatus': 'disabled', 'ClusterNamespaceArn': 'arn:aws:redshift:ap-south-1:681365143658:namespace:7fccb75e-e245-492f-aa48-bd26549d382c', 'TotalStorageCapacityInMegaBytes': 400000, 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}}\n"
     ]
    }
   ],
   "source": [
    "#describe tthe cluster value\n",
    "# retrieves information about a specific Redshift cluster identified by the ClusterIdentifier my-redshift-cluster\n",
    "\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "cluster_info =redshift_client.describe_clusters(ClusterIdentifier='my-redshift-cluster')['Clusters'][0]\n",
    "print(cluster_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8344b230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_table table created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "#pg8000 and psycopg2 are both Python libraries used for interacting with PostgreSQL databases, including Amazon Redshift. \n",
    "#pg8000 is a lightweight and pure-Python PostgreSQL adapter that aims for simplicity and ease of use.\n",
    "#psycopg2 is known for its performance and is often the preferred choice for high-performance database interactions.\n",
    "\n",
    "import pg8000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "redshift_endpoint = 'my-redshift-cluster.cjoyi9o6hfqt.ap-south-1.redshift.amazonaws.com'\n",
    "redshift_port = 5439\n",
    "redshift_user = 'myawsuser'\n",
    "redshift_password = 'Password13'\n",
    "redshift_database = 'mydatabase'\n",
    "redshift_table = 'product_table'\n",
    "\n",
    "# Create a connection to Redshift using pg8000\n",
    "conn = pg8000.connect(host=redshift_endpoint,\n",
    "                      port=redshift_port,\n",
    "                      database=redshift_database,\n",
    "                      user=redshift_user,\n",
    "                      password=redshift_password)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it does not exist\n",
    "create_table_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product_table (\n",
    "marketplace varchar(50),\n",
    "customer_id varchar(50),\n",
    "product_id varchar(50),\n",
    "seller_id varchar(50),\n",
    "sell_date varchar(50),\n",
    "quantity integer\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the create table command\n",
    "    cursor.execute(create_table_command)\n",
    "    conn.commit()\n",
    "    print('product_table table created successfully or already exists.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error creating table:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b188d030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "#insert data into table using copy command\n",
    "#copy data from s3 bucket into redshift table\n",
    "#s3://mypythonproject1/input/product_data.csv  copy data into redshift table product_table\n",
    "\n",
    "\n",
    "import pg8000\n",
    "\n",
    "\n",
    "# Create a connection to Redshift\n",
    "#execute a copy command in cluster . to copy data from s3://mypythonproject1/input/product_data.csv into redshift table\n",
    "conn = pg8000.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "input_bucket = 'mypythonproject1'\n",
    "input_file_key = 'input/product_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "copy_command = f\"\"\"\n",
    "COPY public.product_table\n",
    "FROM 's3://mypythonproject1/input/product_data.csv'\n",
    "CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_key}'\n",
    "DELIMITER ',' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(copy_command)\n",
    "    conn.commit()\n",
    "    print('COPY command executed successfully.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error executing COPY command:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ef01258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp table created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "#psycopg2 is known for its performance and is often the preferred choice for high-performance database interactions.\n",
    "#create table emp in redshift using psycopg2\n",
    "\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "redshift_endpoint = 'my-redshift-cluster.cjoyi9o6hfqt.ap-south-1.redshift.amazonaws.com'\n",
    "redshift_port = 5439\n",
    "redshift_user = 'myawsuser'\n",
    "redshift_password = 'Password13'\n",
    "redshift_database = 'mydatabase'\n",
    "redshift_table = 'product_table'\n",
    "\n",
    "# Create a connection to Redshift using pg8000\n",
    "conn = psycopg2.connect(host=redshift_endpoint,\n",
    "                      port=redshift_port,\n",
    "                      database=redshift_database,\n",
    "                      user=redshift_user,\n",
    "                      password=redshift_password)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it does not exist\n",
    "create_table_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS emp (\n",
    "emp_id int,\n",
    "name varchar(100),\n",
    "salary decimal\n",
    "\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the create table command\n",
    "    cursor.execute(create_table_command)\n",
    "    conn.commit()\n",
    "    print('emp table created successfully or already exists.')\n",
    "except psycopg2.Error as e:\n",
    "    print('Error creating table:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8836c73e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError_",
     "evalue": "Load into table 'emp' failed.  Check 'stl_load_errors' system table for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError_\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2792/986333914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Execute the COPY command\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy_command\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'COPY command executed successfully.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError_\u001b[0m: Load into table 'emp' failed.  Check 'stl_load_errors' system table for details.\n"
     ]
    }
   ],
   "source": [
    "#copy emp txt file into table emp in redshift using psycopg2\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Create a connection to Redshift\n",
    "#execute a copy command in cluster . to copy data from s3://mypythonproject1/input/product_data.csv into redshift table\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "input_bucket = 'myglue-etl-project'\n",
    "input_file_key = 'input/emp.txt'\n",
    "\n",
    "\n",
    "\n",
    "copy_command = f\"\"\"\n",
    "COPY public.emp\n",
    "FROM 's3://mypythonproject1/input/emp.txt'\n",
    "CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_key}'\n",
    "DELIMITER ',' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(copy_command)\n",
    "    conn.commit()\n",
    "    print('COPY command executed successfully.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error executing COPY command:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print('COPY command executed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "51678283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 106511, datetime.datetime(2023, 7, 9, 18, 35, 42, 200174), 1073782902, 357, 's3://mypythonproject1/input/emp.txt                                                                                                                                                                                                                             ', 2, 'emp_id                                                                                                                         ', 'int4      ', '0         ', 0, '123|John Smith|5000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ', '123|John Smith|5000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ', 1214, 'Delimiter not found                                                                                 ', 0, 0, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>slice</th>\n",
       "      <th>tbl</th>\n",
       "      <th>starttime</th>\n",
       "      <th>session</th>\n",
       "      <th>query</th>\n",
       "      <th>filename</th>\n",
       "      <th>line_number</th>\n",
       "      <th>colname</th>\n",
       "      <th>type</th>\n",
       "      <th>col_length</th>\n",
       "      <th>position</th>\n",
       "      <th>raw_line</th>\n",
       "      <th>raw_field_value</th>\n",
       "      <th>err_code</th>\n",
       "      <th>err_reason</th>\n",
       "      <th>is_partial</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>copy_job_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>106511</td>\n",
       "      <td>2023-07-09 18:35:42.200174</td>\n",
       "      <td>1073782902</td>\n",
       "      <td>357</td>\n",
       "      <td>s3://mypythonproject1/input/emp.txt           ...</td>\n",
       "      <td>2</td>\n",
       "      <td>emp_id                                        ...</td>\n",
       "      <td>int4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123|John Smith|5000                           ...</td>\n",
       "      <td>123|John Smith|5000                           ...</td>\n",
       "      <td>1214</td>\n",
       "      <td>Delimiter not found                           ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  slice     tbl                  starttime     session  query  \\\n",
       "0     100      1  106511 2023-07-09 18:35:42.200174  1073782902    357   \n",
       "\n",
       "                                            filename  line_number  \\\n",
       "0  s3://mypythonproject1/input/emp.txt           ...            2   \n",
       "\n",
       "                                             colname        type  col_length  \\\n",
       "0  emp_id                                        ...  int4        0            \n",
       "\n",
       "   position                                           raw_line  \\\n",
       "0         0  123|John Smith|5000                           ...   \n",
       "\n",
       "                                     raw_field_value  err_code  \\\n",
       "0  123|John Smith|5000                           ...      1214   \n",
       "\n",
       "                                          err_reason  is_partial  \\\n",
       "0  Delimiter not found                           ...           0   \n",
       "\n",
       "   start_offset  copy_job_id  \n",
       "0             0            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check error\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL query to retrieve load error details from 'stl_load_errors'\n",
    "cur.execute(\"SELECT * FROM stl_load_errors\")\n",
    "\n",
    "# Fetch all the rows returned by the query\n",
    "load_errors = cur.fetchall()\n",
    "\n",
    "#Print the load error details\n",
    "for error in load_errors:\n",
    "    print(error)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "#format is not good \n",
    "\n",
    "\n",
    "# Create a dataframe from the load_errors data so convert into dataframe\n",
    "df = pd.DataFrame(load_errors, columns=[desc[0] for desc in cur.description])\n",
    "\n",
    "# Display the dataframe\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b2aa7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "#Now correct DELIMITER and again execute code\n",
    "#we need to use pipe delemiter\n",
    "\n",
    "#copy emp txt file into table emp in redshift using psycopg2\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Create a connection to Redshift\n",
    "#execute a copy command in cluster . to copy data from s3://mypythonproject1/input/product_data.csv into redshift table\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL statements\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "input_bucket = 'myglue-etl-project'\n",
    "input_file_key = 'input/emp.txt'\n",
    "\n",
    "\n",
    "\n",
    "copy_command = f\"\"\"\n",
    "COPY public.emp\n",
    "FROM 's3://mypythonproject1/input/emp.txt'\n",
    "CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_key}'\n",
    "DELIMITER '|' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(copy_command)\n",
    "    conn.commit()\n",
    "    print('COPY command executed successfully.')\n",
    "except pg8000.Error as e:\n",
    "    print('Error executing COPY command:', e)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "86730c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 'John Smith', Decimal('5000'))\n",
      "(456, 'Jane Doe', Decimal('7500'))\n",
      "(789, 'Alan Johnson', Decimal('10000'))\n"
     ]
    }
   ],
   "source": [
    "#now select data of emp and product from redshift \n",
    "#query into emp and product_table in reshift\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SELECT query\n",
    "cur.execute(\"SELECT * FROM emp\")\n",
    "\n",
    "# Fetch all the rows returned by the query\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Process the retrieved rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a85aceab",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedTable",
     "evalue": "relation \"product_table\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2792/104260188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Execute the SELECT query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM product_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Fetch all the rows returned by the query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUndefinedTable\u001b[0m: relation \"product_table\" does not exist\n"
     ]
    }
   ],
   "source": [
    "#now select data of emp and product from redshift \n",
    "#query into emp and product_table in reshift\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SELECT query\n",
    "cur.execute(\"SELECT * FROM product_table\")\n",
    "\n",
    "# Fetch all the rows returned by the query\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Process the retrieved rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f214d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop table successfully.\n"
     ]
    }
   ],
   "source": [
    "#Now drop Mutiple table at once \n",
    "#drop table emp and product_table in reshift\n",
    "import psycopg2\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_endpoint,\n",
    "    port=redshift_port,\n",
    "    database=redshift_database,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List of tables to drop\n",
    "tables_to_drop = ['emp', 'product_table']\n",
    "\n",
    "# Drop the tables\n",
    "for table_name in tables_to_drop:\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "print('Drop table successfully.')    \n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "00a0f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete cluster successfully\n"
     ]
    }
   ],
   "source": [
    "#once you done delete your redshift cluster\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "cluster_identifier = 'my-redshift-cluster'\n",
    "\n",
    "# Delete the Redshift cluster\n",
    "redshift_client.delete_cluster(ClusterIdentifier=cluster_identifier,\n",
    "                              SkipFinalClusterSnapshot=True)\n",
    "\n",
    "\n",
    "redshift_client.get_waiter('cluster_deleted').wait(ClusterIdentifier=cluster_identifier)\n",
    "\n",
    "print(\"delete cluster successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6033f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1deeecd2-fc8c-4ad1-b0bb-fac1f29d857a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1deeecd2-fc8c-4ad1-b0bb-fac1f29d857a',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '232',\n",
       "   'date': 'Sun, 09 Jul 2023 18:50:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete the subnet group 'my-subnet-group'  which you created above:\n",
    "#subnet_group_name = 'my-subnet-group'\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift_client = boto3.client('redshift', \n",
    "                               aws_access_key_id=access_key, \n",
    "                               aws_secret_access_key=secret_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Delete the subnet group\n",
    "redshift_client.delete_cluster_subnet_group(ClusterSubnetGroupName=subnet_group_name)\n",
    "\n",
    "\n",
    "#A status code of 200 indicates that the request was successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0de5f669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '30263aef-ad53-485a-97e8-6cb6f54f0a5b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '30263aef-ad53-485a-97e8-6cb6f54f0a5b',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'content-length': '239',\n",
       "   'date': 'Sun, 09 Jul 2023 18:51:37 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete security group  security_group_id  which you created above\n",
    "\n",
    "\n",
    "ec2_client = boto3.client('ec2', \n",
    "                          aws_access_key_id=access_key, \n",
    "                          aws_secret_access_key=secret_key)\n",
    "\n",
    "# Delete the security group\n",
    "ec2_client.delete_security_group(GroupId=security_group_id)\n",
    "\n",
    "\n",
    "#'HTTPStatusCode' field in the response. If the value is 200, it indicates a successful \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab9a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
